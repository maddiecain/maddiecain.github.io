---
layout: post
title:  "Basics of Tensor Networks"
date:   2022-01-07 15:47:04 -0500
categories: jekyll update
---

I'm currently reading [an introduction to tensor networks by Roman Orus][tn-review]. Previously I was introduced to matrix product states (MPS) in a quantum information class, but I have never learned about tensor networks in depth before. First, I'll give a general overview of the benefits and challenges of tensor networks, then I'll dive into more involved details of states that can be represented as tensor networks.

Tensor networks are a condensed way of representing a quantum state without storing the $$p^n$$ wavefunction coefficients, where $$p$$ is the dimension of a qudit and $$n$$ is the number of qudits. The general data structure for a tensor network is a graph: onsite tensors and bond tensors (represented by nodes) and edges, which represent that the tensor indices need to be contracted to evaluate the particular quantity represented by the tensor network. Dangling edges only connected to a single node represent a tensor index that remains after the contraction. The dimension of each index on the tensors is called the bond dimension, denoted $$d$$. The method of encoding the state in locally connected tensors naturally encodes the entanglement structure of the state, and works particularly well for states without long-range entanglement. In one dimension, where the MPS representation efficiently can encode all states with area law entanglement, which includes all thermal and ground states of gapped Hamiltonians in 1D. 

While tensor networks cut down on the space required to store a quantum state, the time required to contract a tensor network can be unwieldy in general. A one-dimensional MPS can be contracted efficiently; for example, to compute an inner product requires time $$O(npd^3)$$. Furthermore, tensor networks representing infinite systems with translational symmetry can be efficiently contracted by computing the dominant eigenvalue(s) and left/right eigenvalues of the transfer matrices, which encompass the "unit cell" of repeating pattern of onsite and bond tensors. Computing observables can be even further simplified by putting the MPS in canonical form. Canonical form involves alternating onsite and bond tensors, where the bond tensors $$\lambda_{\alpha_i}^{[i]}$$ are the Schmidt coefficients that result from taking the Schmidt decomposition between qudit $$i$$ and all qudits to the right of $$i$$. By repeatedly taking Schmidt decompositions from left to right, the MPS can be put in canonical form. By construction, taking expectation values of local operators can be done without contracting the entire MPS (see Figure 1). 

While contracting a MPS only takes time polynomial in the system size and bond dimension, it is important to stoll optimize contraction algorithms, because the contraction runtime can be a high polynoomial in the bond dimension (see Figure 2).
![Tensor network contraction order affects runtime](/assets/images/contraction_order_matters.png)
However, in two dimensions, contracting a PEPS is known to be #P hard. As a reminder, #P involves counting the solutions to NP-Complete problems, so it encompasses all of NP.  Therefore, the problem grows generally intractible in higher dimensions, and clever contraction algorithms must be used. Additionally, clever tricks like putting the PEPS in canonical form are generally intractible in higher dimensions. 

Finding ground states via imaginary time evolution on a MPS or PEPS state is often done with time-evolving block decimation (TEBD), which works by Trotterizing the time evolution so that local operators can be applied sequentially, in parallel, or using a matrix product operator (MPO) to the state. Typically, doing so will increase the bond dimension. To keep the bond dimension under control, a minimization procedure is done to minimize the error between the evolved state and an approximate state with bond dimension $$d$$, so the overall bond dimension does not increase but at the cost some accuracy. The minimization works tensor-by-tensor to minimize the overlap that the time-evolved tensor has in the space of bond dimensions larger than $$d$$. It turns out that the minimization problem can be solved exactly by solving a system of linear equations associated with the point of zero derivative. While TEBD works well for gapped phases, numerical evidence suggests that it becomes unstable near quantum critical points.

Finding variational wavefunctions also works using a tensor-by-tensor energy minimization procedure. Here, we must minimize the expectation of the Hamiltonian over the family of states parametrized by a tensor network with our chosen bond dimension. Because every matrix element of every tensor is a variational parameter, it is not practical to optimize over every matrix element at once. Instead, as in TEBD, one minimizes the energy over one tensor at a time, and sweeps through all tensors several times. In one dimension with open boundary conditions, this procedure is known as density matrix renormalization group (DMRG).

[tn-review]: https://arxiv.org/pdf/1306.2164.pdf

