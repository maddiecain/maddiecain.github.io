---
layout: post
title:  "Basics of Tensor Networks"
date:   2022-01-07 15:47:04 -0500
categories: jekyll update
---
{% include ../mathjax.html %}

I'm currently reading [an introduction to tensor networks by Roman Orus][tn-review]. Previously I was introduced to matrix product states (MPS) in a quantum information class, but I have never learned about tensor networks in depth before. First, I'll give a general overview of the benefits and challenges of tensor networks, then I'll dive into more involved details of states that can be represented as tensor networks.

Tensor networks are a condensed way of representing a quantum state without storing the $$p^n$$ wavefunction coefficients, where $$p$$ is the dimension of a qudit and $$n$$ is the number of qudits. The general data structure for a tensor network is a graph: onsite tensors and bond tensors (represented by nodes) and edges, which represent that the tensor indices need to be contracted to evaluate the particular quantity represented by the tensor network. Dangling edges only connected to a single node represent a tensor index that remains after the contraction. The dimension of each index on the tensors is called the bond dimension, denoted $$d$$. The method of encoding the state in locally connected tensors naturally encodes the entanglement structure of the state, and works particularly well for states without long-range entanglement. In one dimension, where the MPS representation efficiently can encode all states with area law entanglement, which includes all thermal and ground states of gapped Hamiltonians in 1D. 

While tensor networks cut down on the space required to store a quantum state, the time required to contract a tensor network can be unwieldy in general. A one-dimensional MPS can be contracted efficiently; for example, to compute an inner product requires time $$O(npd^3)$$. Furthermore, tensor networks representing infinite systems with translational symmetry can be efficiently contracted by computing the dominant eigenvalue(s) and left/right eigenvalues of the transfer matrices, which encompass the "unit cell" of repeating pattern of onsite and bond tensors. Computing observables can be even further simplified by putting the MPS in canonical form. Canonical form involves alternating onsite and bond tensors, where the bond tensors $$\lambda_{\alpha_i}^{[i]}$$ are the Schmidt coefficients that result from taking the Schmidt decomposition between qudit $$i$$ and all qudits to the right of $$i$$. By repeatedly taking Schmidt decompositions from left to right, the MPS can be put in canonical form. By construction, taking expectation values of local operators can be done without contracting the entire MPS (see Figure 1). While contracting a MPS only takes time polynomial in the system size and bond dimension, it is important to stoll optimize contraction algorithms, because the contraction runtime can be a high polynoomial in the bond dimension (see Figure 2).

However, in two dimensions, contracting a PEPS is known to be #P hard. As a reminder, #P involves counting the solutions to NP-Complete problems, so it encompasses all of NP.  Therefore, the problem grows generally intractible in higher dimensions, and clever contraction algorithms must be used. Additionally, clever tricks like putting the PEPS in canonical form are generally intractible in higher dimensions. 

[tn-review]: https://arxiv.org/pdf/1306.2164.pdf
